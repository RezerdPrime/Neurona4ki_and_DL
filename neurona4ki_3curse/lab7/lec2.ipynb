{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hsbyb4tyki9nx32utdtjpk",
    "id": "71AQJg3CDMn9"
   },
   "source": [
    "# Deep learning для классификации картинок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семинар состоит из 2 частей\n",
    "\n",
    "1. Ознакомьтесь со структурой обычного обучающего скрипта и потренируйте старую добрую сеть, подобную vgg\n",
    "2. Улучшите качество с помощью сети, подобной resnet\n",
    "\n",
    "Но сначала посмотрим на данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./vgg-neural-network-architecture.png\" alt=\"Drawing\" style=\"width:90%\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "34l2kkk7t84llsmzyxus1",
    "id": "2MaELIpIDMoA"
   },
   "source": [
    "# Tiny ImageNet dataset\n",
    "На этом семинаре мы сосредоточимся на задаче распознавания изображений на Tiny Image Net dataset. Этот набор данных содержит\n",
    "* 100 тысяч изображений размером 3x64x64\n",
    "* 200 различных классов: змеи, пауки, кошки, грузовики, кузнечики, чайки и т.д.\n",
    "\n",
    "На самом деле, это подмножество набора данных ImageNet с изображениями, уменьшенными в 4 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "5nh892g5zpl9qv5fki8vpk",
    "id": "5rQhiYyRDMoG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset was downloaded to '.\\tiny-imagenet-200.zip'\n",
      "Extract downloaded dataset to '.'\n"
     ]
    }
   ],
   "source": [
    "from tiny_img import download_tinyImg200\n",
    "data_path = '.'\n",
    "download_tinyImg200(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Training script structure and vgg-like network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы обучить нейронную сеть, надо решить 5 задач:\n",
    "1. data loader (data provider) - как загружать и дополнять данные для обучения nn\n",
    "2. neural network architecture - что будет обучаться\n",
    "3. loss function (+ auxilary metrics on train and validation set) - как проверить качество нейронной сети\n",
    "4. optiimzer and training schedule - как будет обучаться нейронная сеть\n",
    "5. \"Train loop\" - что именно нужно делать для каждого пакета, как часто проверять ошибку проверки, как часто сохранять сеть и т.д. Этот код можно было бы написать в общем виде и повторно использовать в разных сценариях обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "g2i37mixtk9kkxkki1y8",
    "id": "rS_-00tYDMoB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our main computing device is 'cpu'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "def get_computing_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_computing_device()\n",
    "print(f\"Our main computing device is '{device}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data loader and data augmentation\n",
    "Обычно для манипулирования данными используются две связанные абстракции:\n",
    "- Dataset (`torch.utils.data.Dataset` и его подклассы из `torchvision.datasets\") - некоторый черный ящик, который хранит и предварительно обрабатывает отдельные элементы dataset. В частности, на этом уровне обычно находятся дополнения для отдельных образцов.\n",
    "- DataLoader (`torch.utils.data.DataLoader`) - структура, объединяющая отдельные элементы в пакетном режиме.\n",
    "\n",
    "Давайте разберемся с обучающим набором данных. Вот несколько простых дополнений, которые мы собираемся использовать в наших экспериментах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.RandomRotation(5),\n",
    "     # Color jittering transform\n",
    "     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для набора обучающих данных мы будем использовать пользовательский набор данных, который будет хранить все обучающие данные в оперативной памяти. Если у вас недостаточно оперативной памяти, вы можете использовать `torch vision.datasets.ImageFolder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "jrzsbgniodgtg1hif324k9",
    "id": "5vq5Cm0ADMoK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiny-imagenet-200/train: 100%|██████████| 50/50 [00:40<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import tiny_img_dataset\n",
    "# you may use torchvision.datasets.ImageFolder() with the same parameters for loading train dataset \n",
    "train_dataset = tiny_img_dataset.TinyImagenetRAM('tiny-imagenet-200/train', transform=train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверка. Взгляните на папку `tiny-imagenet-200/val` и сравните ее с папкой `tiny-imagenet-200/train`. Выглядит по-другому, не так ли? Таким образом, мы не можем использовать `TinyImagenetRAM` для загрузки набора данных для проверки. Давайте вместо этого напишем пользовательский набор данных, но с таким же поведением, как у `TinyImagenetRAM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def read_rgb_image(path):\n",
    "    \"\"\"Helper function to read an image as RGB numpy array\"\"\"\n",
    "    with Image.open(path) as img:\n",
    "        img = img.convert('RGB')\n",
    "        return np.array(img)\n",
    "\n",
    "class TinyImagenetValDataset(Dataset):\n",
    "    def __init__(self, root, transform=transforms.ToTensor()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        with open(os.path.join(root, 'val_annotations.txt')) as f:\n",
    "            annotations = []\n",
    "            for line in f:\n",
    "                img_name, class_label = line.split('\\t')[:2]\n",
    "                annotations.append((img_name, class_label))\n",
    "\n",
    "        # 1. define self.classes - list of sorted class labels from annotations\n",
    "        # Get unique class labels and sort them\n",
    "        unique_classes = sorted({item[1] for item in annotations})\n",
    "        self.classes = unique_classes\n",
    "        \n",
    "        assert len(self.classes) == 200, len(self.classes)\n",
    "        assert all(self.classes[i] < self.classes[i+1] for i in range(len(self.classes)-1)), 'classes should be ordered'\n",
    "        assert all(isinstance(elem, type(annotations[0][1])) for elem in self.classes), 'your just need to reuse class_labels'\n",
    "\n",
    "        # 2. self.class_to_idx - dict from class label to class index\n",
    "        self.class_to_idx = {item: index for index, item in enumerate(self.classes)}\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images, self.targets = [], []\n",
    "        for img_name, class_name in tqdm.tqdm(annotations, desc=root):\n",
    "            img_path = os.path.join(root, 'images', img_name)\n",
    "            # 3. load image and store it in self.images\n",
    "            image = read_rgb_image(img_path)\n",
    "            \n",
    "            assert image.shape == (64, 64, 3), image.shape\n",
    "            self.images.append(Image.fromarray(image))\n",
    "            self.targets.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # take image and its target label from \"self.images\" and \"self.targets\", \n",
    "        # transform the image using self.transform and return the transformed image and its target label\n",
    "        image = self.images[index]\n",
    "        image = self.transform(image)\n",
    "        target = self.targets[index]\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте, наконец, загрузим набор данных для проверки. Обычно вам не применять аугментации для проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiny-imagenet-200/val: 100%|██████████| 5000/5000 [00:01<00:00, 3925.04it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = TinyImagenetValDataset('tiny-imagenet-200/val', transform=transforms.ToTensor())\n",
    "\n",
    "assert all(train_dataset.classes[i] == val_dataset.classes[i] for i in range(50)), \\\n",
    "    'class order in train and val datasets should be the same'\n",
    "assert all(train_dataset.class_to_idx[elem] == val_dataset.class_to_idx[elem] for elem in train_dataset.classes), \\\n",
    "    'class indices should be the same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для большинства случаев будет достаточно `DataLoader` по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "6md8io0fesfby4r9per3jb",
    "id": "tY6OUeOODMoN"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "hsq566ut87vokpkiq68",
    "id": "HBgW-gzwDMoQ"
   },
   "outputs": [],
   "source": [
    "val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fxzxgbl11g2dixss4t9nx",
    "id": "arxSyhBLDMoX"
   },
   "source": [
    "### 1.2 Определение нейронной сети\n",
    "\n",
    "\"Сеть, подобная VGG\", обычно означает, что сеть представляет собой последовательность сверток с MaxPooling для понижающей размерности. Вот таблица из оригинальной статьи [\"Very Deep Convolutional Networks for Large-Scale Image Recognition\"].(https://arxiv.org/abs/1409.1556), который описывает классические конфигурации сетей VGG (часто называемые VGG-A, VGG-B и т.д. С использованием имени столбца в качестве идентификатора или VGG16, VGG19 и т.д. с использованием количества слоев в качестве идентификатора).\n",
    "\n",
    "![image.png](https://pytorch.org/assets/images/vgg.png)\n",
    "\n",
    "Эти сетевые конфигурации были разработаны для набора данных ImageNet. Поскольку изображения в tiny-imagenet имеют пониженный размер в 4 раза, мы собираемся разработать нашу собственную конфигурацию, уменьшив: \n",
    "1) количество слоев;\n",
    "2) количество нейронов в слоях;\n",
    "3) количество слоев с максимальным объединением, которые уменьшают выборку карт объектов\n",
    "\n",
    "Конфигурация нашей сети будет выглядеть следующим образом [Conv(16), Conv(16), MaxPool] + [Conv(32), Conv(32), MaxPool] + [Conv(64), Conv(64), MaxPool] + [Conv(128), Conv(128)] + [GlobalAveragePooling] + [FC(200) + softmax]\n",
    "\n",
    "\n",
    "Мы используем Conv(128) и GlobalAveragePooling вместо image flattening и слоев FC для уменьшения количества параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "g5yf9z66xdpvq688ze2d8",
    "id": "7QF2hMVxDMoY"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6yn15hpuolcmryork2oqs",
    "id": "DJ6QKG3hDMoa"
   },
   "source": [
    "И еще кое-что. VGG был разработан до того, как был придуман BatchNormalization. В настоящее время было бы глупо, если бы мы не использовали BatchNormalization в нашей сети. Итак, давайте определим простой модуль, содержащий свертку, BatchNormalization и relu, и построим нашу сеть, используя этот модуль. Вот также реализация GlobalAveragePooling, приведенная для вас в качестве примера пользовательского модуля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "f985tf2dvssqwmyc6w99d",
    "id": "u_mbfRXMDMob"
   },
   "outputs": [],
   "source": [
    "#L!\n",
    "class GlobalAveragePool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=self.dim)\n",
    "\n",
    "    \n",
    "class ConvBNRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding='same'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define convolution, batchnorm, relu\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding if isinstance(padding, int) else (kernel_size // 2),\n",
    "            bias=False  # Batchnorm will handle bias\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Sequentially apply convolution, batchnorm, relu to 'x'\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def create_vgg_like_network(config=None):\n",
    "    \"\"\"\n",
    "    Creates VGG like network according to config\n",
    "    \"\"\"\n",
    "    model = nn.Sequential()\n",
    "    \n",
    "    default_config = [[16,16], [32, 32], [64, 64], [128, 128]]\n",
    "    config = config or default_config\n",
    "    \n",
    "    in_channels = 3\n",
    "    for block_index in range(len(config)):\n",
    "        for layer_index_in_block in range(len(config[block_index])):\n",
    "            out_channels = config[block_index][layer_index_in_block]\n",
    "            \n",
    "            # Add ConvBNRelu module to model\n",
    "            model.add_module(\n",
    "                f'conv_{block_index}_{layer_index_in_block}',\n",
    "                ConvBNRelu(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding='same'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            \n",
    "        if block_index != len(config) - 1:\n",
    "            model.add_module(f'mp_{block_index}', nn.MaxPool2d(3, stride=2))\n",
    "            \n",
    "    model.add_module('pool', GlobalAveragePool(dim=(2,3)))\n",
    "    model.add_module('logits', nn.Linear(out_channels, 50))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот и создана наша модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vgg_like_network()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7dh3d8xmkeinv4kx0g079",
    "id": "DvugZZbeDMoe"
   },
   "source": [
    "### 1.3 Определение функции потерь\n",
    "\n",
    "Обычно в качестве функции потерь для классификации изображений используется cross-entropy (отрицательное логарифмическое правдоподобие)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "3y7p7o6s7vecpf3kpktj8v",
    "id": "cGEhRWMYDMof"
   },
   "outputs": [],
   "source": [
    "def compute_loss(predictions, gt):\n",
    "    return F.cross_entropy(predictions, gt).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Optimizer and training schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте обучим нашу сеть, используя Adam с параметрами по умолчанию. \n",
    "\n",
    "Для обучения с помощью `torch.optim.SGD` вам обычно нужно определить training schedule - способ, как снизить learning rate во время тренировки. Но поскольку в adam все градиенты масштабируются по моменту, эффект от правильного графика тренировок не так важен для обучения, как в SGD. Поэтому мы будем действовать как ленивые специалисты по обработке данных и не будем использовать шедулер вообще. Но вы можете поиграть с шедулером, используя, например, `torch.optim.lr_scheduler.ExponentialLR`, смотрите [документацию](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) с объяснением, как это использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Цикл обучения\n",
    "\n",
    "Давайте объединим ранее определенные элементы вместе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellId": "w8rht9ygh7uns89ypozln",
    "id": "sEy0LiHxDMol",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def eval_model(model, data_generator):\n",
    "    accuracy = []\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_generator:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            y_pred = logits.max(1)[1].data\n",
    "            accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy()))\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "            \n",
    "def train_model(model, optimizer, train_data_generator, compute_loss):\n",
    "    train_loss = []\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for (X_batch, y_batch) in tqdm.tqdm(train_data_generator, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        predictions = model(X_batch)\n",
    "        loss = compute_loss(predictions, y_batch)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # metrics\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def train_loop(model, optimizer, train_data_generator, val_data_generator, num_epochs, compute_loss):\n",
    "    \"\"\"\n",
    "    num_epochs - total amount of full passes over training data\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_model(model, optimizer, train_data_generator, compute_loss)\n",
    "        \n",
    "        val_accuracy = eval_model(model, val_data_generator)\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Обучение\n",
    "\n",
    "Вся подготовка завершена, пора запускать обучение!\n",
    "\n",
    "Обычно после обучения в течение 30 эпох вы должны получить нейронную сеть, которая предсказывает метки с точностью более 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 391/391 [02:37<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, opt, train_batch_gen, val_batch_gen, num_epochs=5, compute_loss=compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Say Hello to ResNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части вам нужно переопределить вашу модель, все остальное останется прежним. Как и в случае с VGG, мы собираемся определить модель, подобную ResNet, а не классическую архитектуру, разработанную для классификации ImageNet.\n",
    "\n",
    "\"ResNet-подобный\" обычно означает, что ваша сеть состоит из \"residual блоков\". Существует два широко используемых типа блоков: с двумя и с тремя свертками:\n",
    "![resnet_blocks](https://miro.medium.com/max/613/1*zS2ChIMwAqC5DQbL5yD9iQ.png)\n",
    "\n",
    "На практике часто используются блоки с тремя свертками, поскольку они позволяют построить более глубокую сеть с меньшими параметрами. Блоки с двумя свертками обычно используются для сравнения с non-residual сетями, особенно с VGG и AlexNet.\n",
    "\n",
    "Вот таблица из статьи \"[Deep Residual Learning for Image Recognition]\"(https://arxiv.org/pdf/1512.03385.pdf), в которой описываются классические конфигурации сетей ResNet. Обычно их называют ResNet-18, ResNet-34 и так далее, используя количество слоев в качестве идентификатора. Обратите внимание, что сети, начиная с ResNet-50, основаны на 3-сверточных блоках. На самом деле ResNet-18 и ResNet-34 были представлены только для сравнения с VGG, в то время как ResNet-50 обычно используется на практике в качестве хорошего бейслайна.\n",
    "\n",
    "![изображение](https://miro.medium.com/max/2400/1*aq0q7gCvuNUqnMHh4cpnIw.png)\n",
    "\n",
    "Как и в случае с VGG, мы собираемся создать нашу собственную конфигурацию для сети. Давайте используем 2-сверточных блока для сравнения с vgg и возьмем сеть типа [Conv7x7 - 32] + [conv32-block, conv32-block] + [conv64-block, conv64-block] + [conv128-block, conv128-block] + [GlobalAveragePooling] + fc200 + softmax\n",
    "\n",
    "По сравнению с ResNet18, мы уменьшили количество фильтров и убрали max-pooling в начале и в последнем наборе сверток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock2(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implements the following function:\n",
    "    \n",
    "    output = relu(F(input) + Residual(input)), where: \n",
    "        Residual(x) = Conv + bn + relu + conv + bn\n",
    "        F(x) = x                                        , if in_channels == out_channels and stride == 1\n",
    "             = Conv1x1(in_channel, out_channel, stride) , otherwise\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
    "        super().__init__()\n",
    "        # Define conv1, bn1, relu1, conv2, bn2 for residual branch computation\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding if isinstance(padding, int) else (kernel_size // 2),\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding if isinstance(padding, int) else (kernel_size // 2),\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv3 = None  # conv for main branch adaptation\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.conv3 = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=stride,\n",
    "                padding=0,\n",
    "                bias=False\n",
    "            )\n",
    "            self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute residual branch\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.relu1(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "        \n",
    "        # Compute main branch\n",
    "        if self.conv3 is not None:\n",
    "            x = self.conv3(x)\n",
    "            x = self.bn3(x)\n",
    "            \n",
    "        result = self.relu2(residual + x)\n",
    "        return result\n",
    "\n",
    "\n",
    "def create_resnet_like_network():\n",
    "    model = nn.Sequential()\n",
    "    \n",
    "    config = [[32, 32], [64, 64], [128, 128]]\n",
    "    model.add_module('init_conv', ConvBNRelu(3, 32, kernel_size=7, stride=2, padding=3))\n",
    "    model.add_module('init_pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    in_channels = 32\n",
    "    for i in range(len(config)):\n",
    "        for j in range(len(config[i])):\n",
    "            out_channels = config[i][j]\n",
    "            stride = 2 if i != 0 and j == 0 else 1\n",
    "            # Add ResNetBlock2 module to model\n",
    "            model.add_module(\n",
    "                f'block_{i}_{j}',\n",
    "                ResNetBlock2(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride,\n",
    "                    padding='same'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            in_channels = out_channels\n",
    "    \n",
    "    model.add_module('pool', GlobalAveragePool((2,3)))\n",
    "    model.add_module('logits', nn.Linear(out_channels, 200))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда давайте потренируем нашу сеть. Обычно после обучения в течение 30 эпох вы должны получить нейронную сеть, которая предсказывает метки с точностью >40% и дает около +1% профита по сравнению с vgg, из предыдущего эксперимента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ResNet model\n",
    "model = create_resnet_like_network()\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Create optimizer (using same settings as previous experiment)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Define loss function\n",
    "compute_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train_loop(\n",
    "    model=model,\n",
    "    optimizer=opt,\n",
    "    train_data_generator=train_batch_gen,\n",
    "    val_data_generator=val_batch_gen,\n",
    "    num_epochs=30,\n",
    "    compute_loss=compute_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы внимательно изучали нашу сеть resnet, то могли заметить, что она имеет почти в 2 раза больше параметров и в 2 раза глубже, чем vgg. Давайте определим сеть, сопоставимую vgg, удвоив количество уровней conv.\n",
    "\n",
    "Наш новый сайт VGG-как архитектура будет [Сопу(16), усл. (16), MaxPool] + [Сопу(32), П(32), П(32), П(32), MaxPool] + [Сопу(64) отн(64) отн(64) отн(64), MaxPool] + [Сопу(128), Сопу(128), Сопу(128), Сопу(128)] + [GlobalAveragePooling] + [ФК(200) + softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_vgg_like_network(config=[[16,16], [32,32,32,32], [64, 64, 64, 64], [128, 128, 128, 128]])\n",
    "model = model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "train_loop(model, opt, train_batch_gen, val_batch_gen, num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видите ли вы выгоду от residual связей? \n",
    "\n",
    "Качество сети vgg в этом эксперименте может быть даже хуже, чем качество сети vgg в первом эксперименте. Это связано с проблемой затухания градиента, которая затрудняет обучение глубоких нейронных сетей без residual связей."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "seminar_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "trash2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "notebookId": "0bd81ca7-4175-4905-a84c-21ed8da72299"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
